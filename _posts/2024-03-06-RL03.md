---
layout: post
title:  "Deep Reinforcement Learning 03 (Intro to RL)"
date:   2024-03-05
desc: "Deep Reinforcement Learning 03 (Intro to RL)"
keywords: "Machine learning"
categories: [Machine learning]
tags: [Machine learning]
icon: icon-html
---

# Deep Reinforcement Learning 03 (Intro to RL)

[![image](https://xaltius.tech/wp-content/uploads/2019/07/Reinforcement-Learning.jpg)](https://www.youtube.com/watch?v=ppN5ORNrMos&list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps&index=6)

> **_Keypoints:_**  

- Definitions of Reinforcement Learning
- The goal of Reinforcement Learning

### Definitions of Reinforcement Learning

<!-- Insert an image -->
<div style="text-align:center"><img src="https://github.com/leishi23/homepage/blob/master/_posts/Screenshot%202024-01-31%20194916.png?raw=true" alt="drawing" width="800" position="center"/></div>

- **Agent**: The learner or decision maker that interacts with the environment.
- **Environment**: The external system with which the agent interacts.
- **State**: A representation of the environment at a given time.
- **Action**: A move made by the agent.
- **Reward**: A scalar feedback signal that indicates which states and actions are good or bad. $r(s_t, a_t)$ 
  - _Delayed reward_: The reward may not be immediate but delayed. This is the core challenge in RL.
- **Policy**: A mapping from states to actions, i.e $ \pi_{\theta}(a_t|o_t)$.
- **Value function**: A prediction of future rewards.
- **Observation**: The data received by the agent from the environment.
  - **Fully observable**: The agent observes all relevant information.
    - $ \pi_{\theta}(a_t|s_t)$ where we use $s_t$ to represent the state because the agent can observe all the information.
  - **Partially observable**: The agent observes only some relevant information.

- **Markov Decision Process (MDP)**: $s$, $a$, $r(s,a)$, $p(s'|s,a)$ defines the **Markov Decision Process (MDP)** ![image](https://github.com/leishi23/homepage/blob/master/_posts/Screenshot%20from%202024-03-05%2013-38-18.png?raw=true)

- **Markov chain**: $M = (S, T)$ where $S$ is the state space and $T$ is the transition matrix, i.e $P(s_{t+1}|s_t)$ the probability of transitioning from state $s_t$ to state $s_{t+1}$.
- **Partially observable MDP**: 
<!-- Use HTML to insert an image -->
<div style="text-align:center"><img src="https://github.com/leishi23/homepage/blob/master/_posts/Screenshot%202024-03-06%20003725.png?raw=true" alt="drawing" width="500" position="center"/></div>


### The goal of Reinforcement Learning
<div style="text-align:center"><img src="https://github.com/leishi23/homepage/blob/master/_posts/Screenshot%202024-03-06%20004756.png?raw=true" alt="drawing" width="600" position="center"/></div>