---
layout: post
title:  "Deep Reinforcement Learning 01"
date:   2024-01-28
desc: "Deep Reinforcement Learning 01"
keywords: "Machine learning"
categories: [Machine learning]
tags: [Machine learning]
icon: icon-html
---

# Deep Reinforcement Learning 01

[![image](https://xaltius.tech/wp-content/uploads/2019/07/Reinforcement-Learning.jpg)](https://www.youtube.com/watch?v=Ufww5pzc_N0&list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps&index=3)

> **_Keypoints:_**  

- Introduction
  - What is Reinforcement Learning?
  - Why Deep Reinforcement Learning?
## Introduction

### What is Reinforcement Learning?
![image](https://miro.medium.com/v2/resize:fit:540/1*6K-oWzwlJ4NWORSLT3Oviw.png)
- Agent: The learner and decision maker
- Environment: Everything outside the agent
- State: The state of the environment
  - Observation: The observation of the agent from the environment. To be more specific, the observation is the state of the environment that the agent can perceive.
- Action: The action of the agent
- Reward: The reward of the agent after taking the action
  
#### Supervised Learning vs Reinforcement Learning
Some problems don't have a clear answer, but we can still train the agent to get the best result. For example, the game of Go. The agent can't know the best move in the current state, but it can learn from the experience and get the best move. However, in supervised learning, we need to know the answer of the problem. For example, the image classification problem. We need to know the label of the image to train the model.

#### Using Machine Learning to Evaluate the Action
Sometimes, there is no clear evaluation of the action. For example, when traing AI to imitate human conservation, we can't know the best answer. However, we can use machine learning to evaluate the action. For example, we can use a neural network to evaluate the action. The input of the neural network is the state of the environment and the output is the evaluation of the action. Then, we can use the evaluation to train the agent.

#### Reward Delay
The reward of the action may be delayed. For example, when playing chess, the reward of the action may be delayed until the end of the game. Therefore, we need to consider the reward delay when training the agent.

### Why Deep Reinforcement Learning?
- Deep `=` scalable learning from large, complex datasets
  - i.e. to use neural network to model the agent or the evaluation of the action
- Reinforcement Learning `=` Optimization

<!-- ## Policy-based & Value-based
Policy-based is to **learn an actor** while value-based is to **learn a critic**. The actor is to decide which action to take while the critic is to evaluate the action. The actor and the critic can be a neural network.

#### Asynchronous Advantage Actor-Critic (A3C)
Machine learning is to look for a function i.e. $Action = \Pi (Observation)$, which is the policy. The policy can be a neural network. The neural network takes the observation as the input and output the action. The neural network can be trained by the reward. The reward is the evaluation of the action. The reward can be calculated by the critic. The critic is also a neural network. The critic takes the observation as the input and output the evaluation of the action. 

- Three steps of deep reinforcement learning
  - Define a set of functions that can represent the policy, which is neural network in DRL.
    - Input of the neural network(Actor): Observation
    - Output of the neural network(Actor): Action. Each action corresponds to a neuron in the output layer. The value of the neuron is the probability of the action. For example, if there are 3 actions, the output layer has 3 neurons. The value of the neuron is the probability of the action.
    - Advantage of the neural network(Actor): The neural network can represent any function which may not be known by us.
  - Goodness of the function is measured by the reward.
  - The function is updated by the reward, i.e. to pick the best function.  -->